from selenium import webdriver
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import datetime
import os

# Load categories from sitemap data generated by getSitemapxml.py
with open("categorySites.json", "r") as f:
    category_sites = json.load(f)

CATEGORIES = {}
for index, path in enumerate(category_sites, start=1):
    category_name = path.split("/")[2]
    # path is like "/category/cameras/cameras", strip "/category/" prefix
    category_path = path.removeprefix("/category/")
    CATEGORIES[str(index)] = {
        "name": category_name,
        "path": category_path,
    }



def get_product_details(driver, product_url):
    """Get detailed information from a product page"""
    driver.get(product_url)

    try:
        # Get product name from the product page
        name_element = WebDriverWait(driver, 1).until(
            EC.presence_of_element_located(
                (
                    By.CSS_SELECTOR,
                    "#productDiplayPage > div > div:nth-child(2) > div.col-12.js-space-save-top.position-relative > div > div.col-12.col-xl-8.col-xxl-9.js-product-header-block.product-header-block > h1",
                )
            )
        )
        product_name = name_element.text.strip()

        # Get detailed specifications
        try:
            specs_container = WebDriverWait(driver, 1).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "#featuresSpecs"))
            )

            specs_dict = {}
            labels = specs_container.find_elements(By.CSS_SELECTOR, "p.label_")
            values = specs_container.find_elements(By.CSS_SELECTOR, "p.value_")

            for label, value in zip(labels, values):
                label_text = label.text.strip().rstrip(":")
                value_text = value.text.strip()
                if value_text and value_text != "â€¦":
                    specs_dict[label_text] = value_text

            detailed_specs = "\n".join(f"{k}: {v}" for k, v in specs_dict.items())
            if not detailed_specs:
                detailed_specs = "Detailed specs not found"
        except Exception as e:
            print(f"Error getting detailed specs: {e}")
            detailed_specs = "Detailed specs not found"

        return {"name": product_name, "detailed_specs": detailed_specs}
    except Exception as e:
        print(f"Error getting product details: {e}")
        return {"name": "Name not found", "detailed_specs": "Detailed specs not found"}


def scrape_page(driver, url):
    """Scrape products from a single page"""
    driver.get(url)

    # Find all product containers
    WebDriverWait(driver, 1).until(
        EC.presence_of_all_elements_located(
            (By.CSS_SELECTOR, "#mainCatList .row.w-100.mx-0.ms-xl-2.me-xl-1 > div")
        )
    )

    product_data = []
    product_containers = driver.find_elements(
        By.CSS_SELECTOR, "#mainCatList .row.w-100.mx-0.ms-xl-2.me-xl-1 > div"
    )
    print(f"Found {len(product_containers)} products")

    for container in product_containers:
        try:
            name_element = container.find_element(
                By.CSS_SELECTOR, ".card-item-header a h2"
            )
            general_specs_element = container.find_element(
                By.CSS_SELECTOR, ".card-item-header a h3"
            )

            name = name_element.text.strip()
            general_specs = general_specs_element.text.strip()
            product_url = container.find_element(
                By.CSS_SELECTOR, ".card-item-header a"
            ).get_attribute("href")

            try:
                price_elements = container.find_elements(
                    By.CSS_SELECTOR, ".priceClass .item-price-amount .ginc span"
                )
                price = (
                    "".join(e.text for e in price_elements)
                    .strip()
                    .replace("$", "")
                    .replace(",", "")
                )
            except Exception as e:
                print(f"Error getting price from list for {name}: {e}")
                price = "Price not found"

            product_data.append(
                {
                    "name": name,
                    "general_specs": general_specs,
                    "url": product_url,
                    "price": price,
                }
            )

        except Exception as e:
            print(f"Error finding product in list: {e}")

    return product_data


def get_total_pages(driver):
    """Get the total number of pages for the category"""
    try:
        pagination_elements = driver.find_elements(
            By.CSS_SELECTOR, ".pagination .page-item .page-link"
        )
        page_numbers = [
            int(e.text.strip()) for e in pagination_elements if e.text.strip().isdigit()
        ]
        return max(page_numbers) if page_numbers else 1
    except Exception as e:
        print(f"Error getting total pages: {e}")
        return 1


def scrape_category(driver, category_path):
    """Scrape all products from a category"""
    products = []
    base_url = f"https://www.pbtech.co.nz/category/{category_path}/shop-all"

    # Get total pages
    driver.get(base_url)
    total_pages = get_total_pages(driver)
    print(f"Found {total_pages} pages for {category_path}")

    # Scrape each page
    for page_num in range(1, total_pages + 1):
        print(f"\nScraping page {page_num}/{total_pages}")
        page_url = f"{base_url}?pg={page_num}#sortGroupForm"

        # Get basic product info from the listing page
        product_data = scrape_page(driver, page_url)

        # Get detailed info for each product
        for product in product_data:
            try:
                details = get_product_details(driver, product["url"])
                products.append(
                    {
                        "Product Name": details["name"],
                        "Category": category_path,
                        "General Specs": product["general_specs"],
                        "Detailed Specs": details["detailed_specs"],
                        "Price": product["price"],
                        "Product URL": product["url"],
                    }
                )
                print(f"Scraped details for: {details['name']}")
            except Exception as e:
                print(f"Error scraping product details: {e}")

    return products


def save_results(products, category_path):
    """Save scraped products to JSON"""
    if not products:
        print("No products to save")
        return

    safe_cat_name = category_path.replace("/", "_")
    current_date = datetime.datetime.now().strftime("%Y-%m-%d")
    json_filename = f"pbtech_{safe_cat_name}_{current_date}.json"

    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(products, f, ensure_ascii=False, indent=4)

    print(f"Successfully scraped {len(products)} products")
    print(f"Data saved to {json_filename}")


def main():
    """Main execution function"""
    all_categories = [cat["path"] for cat in CATEGORIES.values()]
    print(f"\n=== PB Tech Scraper ===")
    print(f"Scraping all {len(all_categories)} categories")

    os.makedirs("data", exist_ok=True)
    os.chdir("data/")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))

    try:
        for category_path in all_categories:
            print(f"\nScraping category: {category_path}")
            products = scrape_category(driver, category_path)
            save_results(products, category_path)
    finally:
        driver.quit()


if __name__ == "__main__":
    main()
